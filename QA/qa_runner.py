#!/usr/bin/env python3
"""
HITS AI Agent QA Runner
AI agentë¥¼ ì‹¤í–‰í•˜ì—¬ QA íƒœìŠ¤í¬ë¥¼ í‰ê°€í•˜ëŠ” CLI ë„êµ¬
"""

import argparse
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from biomni.config import default_config

default_config.llm = "gemini-3-pro-preview"
# default_config.llm = "us.anthropic.claude-sonnet-4-5-20250929-v1:0"
# default_config.llm = "us.anthropic.claude-sonnet-4-20250514-v1:0"
default_config.commercial_mode = True
default_config.use_tool_retriever = True
default_config.path = "/workdir_efs/jaechang/work2/biomni_hits_test/biomni_data"
default_config.timeout_seconds = 3600

# Biomni HITS ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from qa_core import (
    Evaluator,
    ImageComparator,
    QAManager,
    ReportGenerator,
)


def extract_solution_from_response(response: str) -> str:
    """
    AI agent ì‘ë‹µì—ì„œ <solution>...</solution> íƒœê·¸ ë‚´ìš© ì¶”ì¶œ

    Args:
        response: AI agentì˜ ì „ì²´ ì‘ë‹µ

    Returns:
        solution íƒœê·¸ ë‚´ìš© (íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µ ë°˜í™˜)
    """
    # <solution>...</solution> íŒ¨í„´ ì¶”ì¶œ
    pattern = r"<solution>(.*?)</solution>"
    match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)

    if match:
        return match.group(1).strip()
    else:
        # solution íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µ ë°˜í™˜
        print(
            "Warning: <solution> tag not found in agent response. Using full response."
        )
        return response.strip()


def run_agent_on_question(
    agent, question: str, task_id: str, input_data_files: Optional[List[str]] = None
) -> tuple[str, float]:
    """
    AI agentì—ê²Œ ì§ˆë¬¸ì„ ì£¼ê³  ë‹µë³€ ìƒì„±

    Args:
        agent: A1_HITS agent ì¸ìŠ¤í„´ìŠ¤
        question: ì§ˆë¬¸ í…ìŠ¤íŠ¸
        task_id: íƒœìŠ¤í¬ ID
        input_data_files: input data íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (optional)

    Returns:
        (ë‹µë³€, ì‹¤í–‰ì‹œê°„) íŠœí”Œ
    """
    print(f"\n{'='*60}")
    print(f"Running AI Agent on Task: {task_id}")
    if input_data_files:
        print(f"Input Data: {', '.join(input_data_files)}")
    print(f"{'='*60}")

    start_time = time.time()

    try:
        # agent ì‹¤í–‰ (go ì‚¬ìš© - run2.py ìŠ¤íƒ€ì¼)
        full_response_parts = []
        print("\nğŸ¤– Agent is thinking...\n")

        for idx, output in enumerate(agent.go(question)):
            print(f"==================== Step {idx} ====================")

            if idx == 0:
                # ì²« ë²ˆì§¸ ì¶œë ¥ì€ system prompt
                print("System prompt loaded")
                continue

            # Handle structured content (list with images) - extract text only
            if isinstance(output, list):
                # Extract text parts from structured content
                text_parts = [
                    item["text"]
                    for item in output
                    if isinstance(item, dict) and item.get("type") == "text"
                ]
                if text_parts:
                    full_response_parts.extend(text_parts)
            elif isinstance(output, str):
                full_response_parts.append(output)

        # ì „ì²´ ì‘ë‹µ ì¡°í•©
        full_response = "\n".join(full_response_parts)

        # solution íƒœê·¸ ì¶”ì¶œ
        answer = extract_solution_from_response(full_response)

        execution_time = time.time() - start_time

        print(f"\nâœ… Agent completed in {execution_time:.2f}s")
        print(f"Answer length: {len(answer)} characters")

        return answer, execution_time

    except Exception as e:
        execution_time = time.time() - start_time
        print(f"\nâŒ Agent failed after {execution_time:.2f}s: {e}")
        import traceback

        traceback.print_exc()
        return f"Error: {str(e)}", execution_time


def save_agent_output(
    task_id: str, question: str, answer: str, output_dir: Path
) -> Path:
    """
    AI agentì˜ ì¶œë ¥ì„ ì €ì¥

    Args:
        task_id: íƒœìŠ¤í¬ ID
        question: ì§ˆë¬¸ í…ìŠ¤íŠ¸
        answer: ë‹µë³€ í…ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬

    Returns:
        ì €ì¥ëœ ë‹µë³€ íŒŒì¼ ê²½ë¡œ
    """
    task_output_dir = output_dir / task_id
    task_output_dir.mkdir(parents=True, exist_ok=True)

    # ì§ˆë¬¸ ì €ì¥
    question_file = task_output_dir / "question.md"
    question_file.write_text(question, encoding="utf-8")

    # ë‹µë³€ ì €ì¥
    answer_file = task_output_dir / "generated_answer.md"
    answer_file.write_text(answer, encoding="utf-8")

    # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë³µì‚¬ (ì¶”í›„ êµ¬í˜„ í•„ìš” ì‹œ)
    # TODO: agentê°€ ìƒì„±í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ì„œ task í´ë”ë¡œ ë³µì‚¬í•˜ëŠ” ë¡œì§

    print(f"ğŸ“ Saved output to: {task_output_dir}")

    return answer_file


def evaluate_task(
    task_id: str,
    question: str,
    ground_truth: str,
    generated_answer: str,
    ground_truth_task_dir: Path,
    generated_task_dir: Path,
    evaluator: Evaluator,
    image_comparator: ImageComparator,
    execution_time: float,
) -> tuple:
    """
    íƒœìŠ¤í¬ í‰ê°€ ìˆ˜í–‰

    Args:
        task_id: íƒœìŠ¤í¬ ID
        question: ì§ˆë¬¸
        ground_truth: ì •ë‹µ
        generated_answer: ìƒì„±ëœ ë‹µë³€
        ground_truth_task_dir: ì •ë‹µ íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬
        generated_task_dir: ìƒì„±ëœ íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬
        evaluator: Evaluator ì¸ìŠ¤í„´ìŠ¤
        image_comparator: ImageComparator ì¸ìŠ¤í„´ìŠ¤
        execution_time: ì‹¤í–‰ ì‹œê°„

    Returns:
        (evaluation_result, image_evaluation) íŠœí”Œ
    """
    print(f"\nğŸ“Š Evaluating task: {task_id}")

    # í…ìŠ¤íŠ¸ í‰ê°€
    evaluation_result = evaluator.evaluate_answer(
        task_id, question, ground_truth, generated_answer
    )

    # ì´ë¯¸ì§€ í‰ê°€
    image_evaluation = image_comparator.evaluate_images(
        ground_truth_markdown=ground_truth,
        generated_markdown=generated_answer,
        ground_truth_task_dir=ground_truth_task_dir,
        generated_task_dir=generated_task_dir,
        compare_visually=True,
    )

    return evaluation_result, image_evaluation


def run_qa_pipeline(
    qa_manager: QAManager,
    agent,
    evaluator: Evaluator,
    image_comparator: ImageComparator,
    report_generator: ReportGenerator,
    output_base_dir: Path,
    task_ids: Optional[List[str]] = None,
    category: Optional[str] = None,
) -> List[Dict]:
    """
    ì „ì²´ QA íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        qa_manager: QAManager ì¸ìŠ¤í„´ìŠ¤
        agent: AI agent ì¸ìŠ¤í„´ìŠ¤
        evaluator: Evaluator ì¸ìŠ¤í„´ìŠ¤
        image_comparator: ImageComparator ì¸ìŠ¤í„´ìŠ¤
        report_generator: ReportGenerator ì¸ìŠ¤í„´ìŠ¤
        output_base_dir: ê²°ê³¼ ì €ì¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬
        task_ids: ì‹¤í–‰í•  íƒœìŠ¤í¬ ID ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
        category: ì¹´í…Œê³ ë¦¬ í•„í„°

    Returns:
        ëª¨ë“  í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    # ì‹¤í–‰í•  íƒœìŠ¤í¬ ì„ íƒ
    if task_ids:
        tasks = [
            qa_manager.get_task(tid) for tid in task_ids if qa_manager.get_task(tid)
        ]
    else:
        tasks = qa_manager.list_tasks(category=category)

    if not tasks:
        print("âŒ No tasks to run!")
        return []

    print(f"\nğŸš€ Running QA pipeline on {len(tasks)} task(s)")

    # ì‹¤í–‰ ID ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„)
    run_id = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    run_output_dir = output_base_dir / run_id
    run_output_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“ Output directory: {run_output_dir}")

    all_results = []

    for idx, task in enumerate(tasks, 1):
        print(f"\n{'#'*60}")
        print(f"Task {idx}/{len(tasks)}: {task.task_id}")
        print(f"{'#'*60}")

        try:
            # 1. AI Agent ì‹¤í–‰ (input data ì •ë³´ ì „ë‹¬)
            generated_answer, execution_time = run_agent_on_question(
                agent, task.question, task.task_id, task.input_data
            )

            # 2. ì¶œë ¥ ì €ì¥
            task_output_dir = run_output_dir / task.task_id
            save_agent_output(
                task.task_id, task.question, generated_answer, run_output_dir
            )

            # 3. í‰ê°€ ìˆ˜í–‰
            ground_truth_task_dir = task.task_path if task.task_path else Path(".")
            generated_task_dir = task_output_dir

            evaluation_result, image_evaluation = evaluate_task(
                task.task_id,
                task.question,
                task.answer,
                generated_answer,
                ground_truth_task_dir,
                generated_task_dir,
                evaluator,
                image_comparator,
                execution_time,
            )

            # 4. ê°œë³„ íƒœìŠ¤í¬ ë¦¬í¬íŠ¸ ìƒì„±
            report_path = task_output_dir / "evaluation.json"
            report_generator.generate_task_report(
                task.task_id,
                evaluation_result,
                image_evaluation,
                execution_time,
                report_path,
            )

            # 5. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            report_generator.print_task_summary(
                report_generator.load_task_report(report_path)
            )

            # ê²°ê³¼ ìˆ˜ì§‘
            all_results.append(report_generator.load_task_report(report_path))

        except Exception as e:
            print(f"âŒ Error processing task {task.task_id}: {e}")
            import traceback

            traceback.print_exc()

    # 6. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    if all_results:
        summary_report_path = run_output_dir / "summary_report.md"
        report_generator.generate_summary_report(all_results, summary_report_path)

        print(f"\n{'='*60}")
        print(f"âœ… QA Pipeline Completed!")
        print(f"{'='*60}")
        print(f"Total tasks: {len(all_results)}")
        print(
            f"Passed: {sum(1 for r in all_results if r['summary']['overall_passed'])}"
        )
        print(f"Results saved to: {run_output_dir}")

    return all_results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="HITS AI Agent QA Runner")

    parser.add_argument(
        "--qa-datasets-dir",
        type=str,
        default="qa_datasets",
        help="QA datasets directory (default: qa_datasets)",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="qa_results",
        help="Output directory for results (default: qa_results)",
    )

    parser.add_argument(
        "--tasks",
        type=str,
        nargs="*",
        help="Specific task IDs to run (default: all tasks)",
    )

    parser.add_argument(
        "--category",
        type=str,
        help="Filter tasks by category",
    )

    parser.add_argument(
        "--pass-threshold",
        type=float,
        default=70.0,
        help="Pass threshold score (0-100, default: 70)",
    )

    parser.add_argument(
        "--ssim-threshold",
        type=float,
        default=0.8,
        help="SSIM threshold for image comparison (0-1, default: 0.8)",
    )

    parser.add_argument(
        "--evaluation-prompt",
        type=str,
        help="Path to custom evaluation prompt file",
    )

    parser.add_argument(
        "--list-tasks",
        action="store_true",
        help="List all available tasks and exit",
    )

    args = parser.parse_args()

    # ê²½ë¡œ ì„¤ì •
    script_dir = Path(__file__).parent
    qa_datasets_dir = script_dir / args.qa_datasets_dir
    output_dir = script_dir / args.output_dir

    # QA Manager ì´ˆê¸°í™”
    print("ğŸ“¦ Initializing QA Manager...")
    qa_manager = QAManager(qa_datasets_dir)

    if args.list_tasks:
        print("\nğŸ“‹ Available Tasks:")
        print(f"{'='*60}")
        for task in qa_manager.list_tasks():
            print(
                f"  - {task.task_id} (Category: {task.category}, Difficulty: {task.difficulty})"
            )
        print(f"{'='*60}")
        print(f"Total: {qa_manager.get_task_count()} tasks")
        return

    if qa_manager.get_task_count() == 0:
        print("âŒ No tasks found! Please add tasks to the qa_datasets directory.")
        return

    # AI Agent ì´ˆê¸°í™”
    print("\nğŸ¤– Initializing AI Agent...")
    try:
        from biomni.agent.a1_hits import A1_HITS

        agent = A1_HITS()
        print("âœ… AI Agent initialized")
    except Exception as e:
        print(f"âŒ Failed to initialize AI Agent: {e}")
        import traceback

        traceback.print_exc()
        return

    # Evaluator ì´ˆê¸°í™”
    print("\nğŸ“Š Initializing Evaluator...")
    try:
        from biomni.llm import get_llm

        llm_client = get_llm(model=default_config.llm)
        evaluator = Evaluator(llm_client, pass_threshold=args.pass_threshold)
        print("âœ… Evaluator initialized")
    except Exception as e:
        print(f"âŒ Failed to initialize Evaluator: {e}")
        import traceback

        traceback.print_exc()
        return

    # Image Comparator ì´ˆê¸°í™”
    print("\nğŸ–¼ï¸  Initializing Image Comparator...")
    image_comparator = ImageComparator(ssim_threshold=args.ssim_threshold)
    print("âœ… Image Comparator initialized")

    # Report Generator ì´ˆê¸°í™”
    print("\nğŸ“„ Initializing Report Generator...")
    report_generator = ReportGenerator()
    print("âœ… Report Generator initialized")

    # QA íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run_qa_pipeline(
        qa_manager=qa_manager,
        agent=agent,
        evaluator=evaluator,
        image_comparator=image_comparator,
        report_generator=report_generator,
        output_base_dir=output_dir,
        task_ids=args.tasks,
        category=args.category,
    )


if __name__ == "__main__":
    main()
