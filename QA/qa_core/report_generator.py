"""
Report Generator: í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from .evaluator import EvaluationResult
from .image_comparator import ImageEvaluationResult


class ReportGenerator:
    """í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self):
        """ReportGenerator ì´ˆê¸°í™”"""
        pass

    def generate_task_report(
        self,
        task_id: str,
        evaluation_result: EvaluationResult,
        image_evaluation: ImageEvaluationResult,
        execution_time: float,
        output_path: Path,
    ) -> None:
        """
        ê°œë³„ íƒœìŠ¤í¬ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± (JSON)

        Args:
            task_id: íƒœìŠ¤í¬ ID
            evaluation_result: í…ìŠ¤íŠ¸ í‰ê°€ ê²°ê³¼
            image_evaluation: ì´ë¯¸ì§€ í‰ê°€ ê²°ê³¼
            execution_time: ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        # LLM ì´ë¯¸ì§€ í‰ê°€ ê²°ê³¼ í™•ì¸
        llm_image_passed = True
        llm_evals = image_evaluation.llm_image_evaluations
        expected_images = image_evaluation.expected_images
        
        # ì´ë¯¸ì§€ê°€ í•„ìš”í•œ íƒœìŠ¤í¬ì¸ ê²½ìš°ì—ë§Œ LLM í‰ê°€ ì²´í¬
        if expected_images:
            if not llm_evals:
                # ì´ë¯¸ì§€ê°€ í•„ìš”í•œë° LLM í‰ê°€ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨
                llm_image_passed = False
            elif "overall" in llm_evals:
                # ë‹¤ì¤‘ ì´ë¯¸ì§€ ë¹„êµ ê²°ê³¼
                llm_result = llm_evals["overall"]
                if "error" in llm_result:
                    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì‹¤íŒ¨
                    llm_image_passed = False
                elif not llm_result.get("passed", False):
                    # passedê°€ Falseë©´ ì‹¤íŒ¨
                    llm_image_passed = False
            else:
                # ê°œë³„ ì´ë¯¸ì§€ ë¹„êµ: í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ ì „ì²´ ì‹¤íŒ¨
                for img_name, llm_result in llm_evals.items():
                    if "error" in llm_result or not llm_result.get("passed", False):
                        llm_image_passed = False
                        break
        
        report = {
            "task_id": task_id,
            "timestamp": evaluation_result.timestamp.isoformat(),
            "execution_time_seconds": execution_time,
            "text_evaluation": {
                "scores": evaluation_result.scores,
                "overall_score": evaluation_result.overall_score,
                "passed": evaluation_result.passed,
                "llm_feedback": evaluation_result.llm_feedback,
                "metadata": evaluation_result.metadata,
            },
            "image_evaluation": image_evaluation.to_dict(),
            "summary": {
                "overall_passed": (
                    evaluation_result.passed 
                    and image_evaluation.all_images_present 
                    and llm_image_passed
                ),
                "text_score": evaluation_result.overall_score,
                "images_present": image_evaluation.all_images_present,
                "average_image_similarity": image_evaluation.average_similarity,
                "llm_image_passed": llm_image_passed,
            },
        }

        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"Task report saved to: {output_path}")

    def generate_summary_report(
        self, all_results: List[Dict[str, Any]], output_path: Path
    ) -> None:
        """
        ì „ì²´ íƒœìŠ¤í¬ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± (Markdown)

        Args:
            all_results: ëª¨ë“  íƒœìŠ¤í¬ì˜ í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        # í†µê³„ ê³„ì‚°
        total_tasks = len(all_results)
        passed_tasks = sum(1 for r in all_results if r.get("summary", {}).get("overall_passed", False))
        avg_score = (
            sum(r.get("text_evaluation", {}).get("overall_score", 0) for r in all_results) / total_tasks
            if total_tasks > 0
            else 0
        )

        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        report_lines = [
            "# HITS AI Agent QA í‰ê°€ ì¢…í•© ë¦¬í¬íŠ¸",
            "",
            f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**ì´ íƒœìŠ¤í¬ ìˆ˜**: {total_tasks}",
            f"**í†µê³¼ íƒœìŠ¤í¬**: {passed_tasks}/{total_tasks} ({passed_tasks/total_tasks*100:.1f}%)",
            f"**í‰ê·  ì ìˆ˜**: {avg_score:.2f}/100",
            "",
            "---",
            "",
            "## íƒœìŠ¤í¬ë³„ ê²°ê³¼",
            "",
            "| Task ID | í…ìŠ¤íŠ¸ ì ìˆ˜ | ì´ë¯¸ì§€(LLM) | Contentâœ“ | í†µê³¼ ì—¬ë¶€ | í”¼ë“œë°± |",
            "|---------|-------------|-------------|----------|-----------|--------|",
        ]

        for result in sorted(all_results, key=lambda x: x["task_id"]):
            task_id = result["task_id"]
            text_eval = result.get("text_evaluation", {})
            img_eval = result.get("image_evaluation", {})
            summary = result.get("summary", {})

            text_score = text_eval.get("overall_score", 0)
            content_present = img_eval.get("all_images_present", False)
            avg_llm = img_eval.get("average_llm_score")
            passed = summary.get("overall_passed", False)
            feedback = text_eval.get("llm_feedback", "")[:50]  # ì²˜ìŒ 50ìë§Œ

            status_icon = "âœ…" if passed else "âŒ"
            content_icon = "âœ…" if content_present else "âŒ"
            
            # LLM ì´ë¯¸ì§€ ì ìˆ˜ í‘œì‹œ
            llm_display = f"{avg_llm:.0f}" if avg_llm is not None else "N/A"

            report_lines.append(
                f"| {task_id} | {text_score:.1f} | {llm_display} | {content_icon} | {status_icon} | {feedback}... |"
            )

        report_lines.extend(
            [
                "",
                "---",
                "",
                "## ì„¸ë¶€ í†µê³„",
                "",
                "### ì ìˆ˜ ë¶„í¬",
                "",
            ]
        )

        # ì ìˆ˜ ë¶„í¬ ê³„ì‚°
        score_ranges = {"90-100": 0, "80-89": 0, "70-79": 0, "60-69": 0, "0-59": 0}
        for result in all_results:
            score = result.get("text_evaluation", {}).get("overall_score", 0)
            if score >= 90:
                score_ranges["90-100"] += 1
            elif score >= 80:
                score_ranges["80-89"] += 1
            elif score >= 70:
                score_ranges["70-79"] += 1
            elif score >= 60:
                score_ranges["60-69"] += 1
            else:
                score_ranges["0-59"] += 1

        for range_name, count in score_ranges.items():
            report_lines.append(f"- **{range_name}ì **: {count}ê°œ íƒœìŠ¤í¬")

        report_lines.extend(
            [
                "",
                "### ì´ë¯¸ì§€ í‰ê°€ (LLM Content-Based)",
                "",
            ]
        )

        # ì´ë¯¸ì§€ í†µê³„ (LLM ê¸°ë°˜)
        tasks_with_images = sum(
            1 for r in all_results if len(r.get("image_evaluation", {}).get("expected_images", [])) > 0
        )
        tasks_all_content_present = sum(
            1 for r in all_results if r.get("image_evaluation", {}).get("all_images_present", False)
        )

        report_lines.append(f"- **ì´ë¯¸ì§€ í•„ìš” íƒœìŠ¤í¬**: {tasks_with_images}ê°œ")
        report_lines.append(
            f"- **ëª¨ë“  ë‚´ìš© í¬í•¨ (LLM íŒë‹¨)**: {tasks_all_content_present}/{tasks_with_images}ê°œ"
        )

        # LLM ì´ë¯¸ì§€ í‰ê°€ í‰ê· 
        llm_scores = [
            r.get("image_evaluation", {}).get("average_llm_score")
            for r in all_results
            if r.get("image_evaluation", {}).get("average_llm_score") is not None
        ]
        if llm_scores:
            avg_llm_score = sum(llm_scores) / len(llm_scores)
            report_lines.append(f"- **í‰ê·  LLM ì´ë¯¸ì§€ ì ìˆ˜**: {avg_llm_score:.1f}/100")
        
        # SSIMì€ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ
        ssim_scores = [
            r.get("image_evaluation", {}).get("average_similarity")
            for r in all_results
            if r.get("image_evaluation", {}).get("average_similarity") is not None
        ]
        if ssim_scores:
            avg_ssim = sum(ssim_scores) / len(ssim_scores)
            report_lines.append(f"- **ì°¸ê³ : SSIM (íŒŒì¼ëª… ë§¤ì¹­ì‹œ)**: {avg_ssim:.3f}")

        report_lines.extend(["", "---", "", f"*ë¦¬í¬íŠ¸ ìƒì„±: HITS AI Agent QA System*", ""])

        # íŒŒì¼ë¡œ ì €ì¥
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))

        print(f"Summary report saved to: {output_path}")

    def load_task_report(self, report_path: Path) -> Dict[str, Any]:
        """
        íƒœìŠ¤í¬ ë¦¬í¬íŠ¸ ë¡œë“œ

        Args:
            report_path: ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ

        Returns:
            ë¦¬í¬íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        with open(report_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def print_task_summary(self, report: Dict[str, Any]) -> None:
        """
        íƒœìŠ¤í¬ ë¦¬í¬íŠ¸ ìš”ì•½ ì¶œë ¥

        Args:
            report: ë¦¬í¬íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*60}")
        print(f"Task: {report['task_id']}")
        print(f"{'='*60}")

        text_eval = report.get("text_evaluation", {})
        print(f"\nğŸ“Š Text Evaluation:")
        print(f"  - Overall Score: {text_eval.get('overall_score', 0):.1f}/100")
        print(f"  - Content Accuracy: {text_eval.get('scores', {}).get('content_accuracy', 0):.1f}")
        print(f"  - Completeness: {text_eval.get('scores', {}).get('completeness', 0):.1f}")
        print(f"  - Format Compliance: {text_eval.get('scores', {}).get('format_compliance', 0):.1f}")
        print(f"  - Passed: {'âœ… Yes' if text_eval.get('passed', False) else 'âŒ No'}")

        img_eval = report.get("image_evaluation", {})
        print(f"\nğŸ–¼ï¸  Image Evaluation:")
        
        # LLM ê¸°ë°˜ í‰ê°€ (ì£¼ í‰ê°€)
        if img_eval.get("average_llm_score") is not None:
            print(f"\n  [LLM Content-Based Evaluation - Primary]")
            print(f"  - LLM Score: {img_eval.get('average_llm_score', 0):.1f}/100")
            print(
                f"  - All Content Present: {'âœ… Yes' if img_eval.get('all_images_present', False) else 'âŒ No'}"
            )
        else:
            print(f"\n  [Basic Check]")
            print(f"  - Expected Images (by name): {len(img_eval.get('expected_images', []))}")
            print(f"  - Found Images: {len(img_eval.get('found_images', []))}")
            print(
                f"  - All Images Present: {'âœ… Yes' if img_eval.get('all_images_present', False) else 'âŒ No'}"
            )
        
        # ì°¸ê³  ì •ë³´
        print(f"\n  [Reference Info]")
        print(f"  - Expected filenames: {img_eval.get('expected_images', [])}")
        print(f"  - Generated filenames: {img_eval.get('found_images', [])}")
        if img_eval.get("average_similarity") is not None:
            print(f"  - SSIM (if name matched): {img_eval.get('average_similarity', 0):.3f}")
            
        # LLM ì´ë¯¸ì§€ í‰ê°€ ì„¸ë¶€ ë‚´ìš©
        llm_evals = img_eval.get("llm_image_evaluations", {})
        if llm_evals:
            print(f"\n  ğŸ“Š LLM Image Content Comparison:")
            # "overall" í‚¤ê°€ ìˆìœ¼ë©´ ë‹¤ì¤‘ ì´ë¯¸ì§€ ë¹„êµ ê²°ê³¼
            if "overall" in llm_evals:
                llm_result = llm_evals["overall"]
                if "error" in llm_result:
                    print(f"    âŒ Error: {llm_result['error']}")
                else:
                    score = llm_result.get("score", 0)
                    passed = llm_result.get("passed", False)
                    all_content = llm_result.get("all_content_present", False)
                    status = "âœ…" if passed else "âŒ"
                    content_status = "âœ…" if all_content else "âŒ"
                    
                    print(f"    â€¢ Overall Score: {status} {score:.1f}/100")
                    print(f"    â€¢ All Content Present: {content_status}")
                    
                    matching = llm_result.get("matching_details", "")
                    if matching:
                        print(f"    â€¢ Matching: {matching}")
                    
                    feedback = llm_result.get("feedback", "")
                    if feedback:
                        # í”¼ë“œë°±ì„ ì§§ê²Œ ì¶œë ¥ (ì²« 150ì)
                        short_feedback = feedback[:150] + "..." if len(feedback) > 150 else feedback
                        print(f"    â€¢ Feedback: {short_feedback}")
            else:
                # ê°œë³„ ì´ë¯¸ì§€ ë¹„êµ ê²°ê³¼ (êµ¬ë²„ì „ í˜¸í™˜)
                for img_name, llm_result in llm_evals.items():
                    if "error" in llm_result:
                        print(f"    â€¢ {img_name}: âŒ {llm_result['error']}")
                    else:
                        score = llm_result.get("score", 0)
                        passed = llm_result.get("passed", False)
                        status = "âœ…" if passed else "âŒ"
                        print(f"    â€¢ {img_name}: {status} {score:.1f}/100")
                        feedback = llm_result.get("feedback", "")
                        if feedback:
                            # í”¼ë“œë°±ì„ ì§§ê²Œ ì¶œë ¥ (ì²« 100ì)
                            short_feedback = feedback[:100] + "..." if len(feedback) > 100 else feedback
                            print(f"      â†’ {short_feedback}")

        print(f"\nğŸ’¬ LLM Feedback:")
        print(f"  {text_eval.get('llm_feedback', 'No feedback')}")

        summary = report.get("summary", {})
        overall_passed = summary.get("overall_passed", False)
        llm_image_passed = summary.get("llm_image_passed", True)
        
        print(f"\n{'='*60}")
        print(f"Overall Result: {'âœ… PASSED' if overall_passed else 'âŒ FAILED'}")
        
        # ì‹¤íŒ¨ ì›ì¸ í‘œì‹œ
        if not overall_passed:
            reasons = []
            if not text_eval.get("passed", False):
                reasons.append("Text evaluation failed")
            if not img_eval.get("all_images_present", False):
                reasons.append("Image content incomplete (LLM)")
            if not llm_image_passed:
                reasons.append("LLM image evaluation failed/error")
            if reasons:
                print(f"Failure reasons: {', '.join(reasons)}")
        
        print(f"{'='*60}\n")

