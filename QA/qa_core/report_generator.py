"""
Report Generator: í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from .evaluator import EvaluationResult
from .image_comparator import ImageEvaluationResult


class ReportGenerator:
    """í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self):
        """ReportGenerator ì´ˆê¸°í™”"""
        pass

    def generate_task_report(
        self,
        task_id: str,
        evaluation_result: EvaluationResult,
        image_evaluation: ImageEvaluationResult,
        execution_time: float,
        output_path: Path,
    ) -> None:
        """
        ê°œë³„ íƒœìŠ¤í¬ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± (JSON)

        Args:
            task_id: íƒœìŠ¤í¬ ID
            evaluation_result: í…ìŠ¤íŠ¸ í‰ê°€ ê²°ê³¼
            image_evaluation: ì´ë¯¸ì§€ í‰ê°€ ê²°ê³¼
            execution_time: ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        report = {
            "task_id": task_id,
            "timestamp": evaluation_result.timestamp.isoformat(),
            "execution_time_seconds": execution_time,
            "text_evaluation": {
                "scores": evaluation_result.scores,
                "overall_score": evaluation_result.overall_score,
                "passed": evaluation_result.passed,
                "llm_feedback": evaluation_result.llm_feedback,
                "metadata": evaluation_result.metadata,
            },
            "image_evaluation": image_evaluation.to_dict(),
            "summary": {
                "overall_passed": evaluation_result.passed and image_evaluation.all_images_present,
                "text_score": evaluation_result.overall_score,
                "images_present": image_evaluation.all_images_present,
                "average_image_similarity": image_evaluation.average_similarity,
            },
        }

        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"Task report saved to: {output_path}")

    def generate_summary_report(
        self, all_results: List[Dict[str, Any]], output_path: Path
    ) -> None:
        """
        ì „ì²´ íƒœìŠ¤í¬ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± (Markdown)

        Args:
            all_results: ëª¨ë“  íƒœìŠ¤í¬ì˜ í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        # í†µê³„ ê³„ì‚°
        total_tasks = len(all_results)
        passed_tasks = sum(1 for r in all_results if r.get("summary", {}).get("overall_passed", False))
        avg_score = (
            sum(r.get("text_evaluation", {}).get("overall_score", 0) for r in all_results) / total_tasks
            if total_tasks > 0
            else 0
        )

        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        report_lines = [
            "# HITS AI Agent QA í‰ê°€ ì¢…í•© ë¦¬í¬íŠ¸",
            "",
            f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**ì´ íƒœìŠ¤í¬ ìˆ˜**: {total_tasks}",
            f"**í†µê³¼ íƒœìŠ¤í¬**: {passed_tasks}/{total_tasks} ({passed_tasks/total_tasks*100:.1f}%)",
            f"**í‰ê·  ì ìˆ˜**: {avg_score:.2f}/100",
            "",
            "---",
            "",
            "## íƒœìŠ¤í¬ë³„ ê²°ê³¼",
            "",
            "| Task ID | ì „ì²´ ì ìˆ˜ | í…ìŠ¤íŠ¸ ì ìˆ˜ | ì´ë¯¸ì§€ | í†µê³¼ ì—¬ë¶€ | í”¼ë“œë°± |",
            "|---------|-----------|-------------|--------|-----------|--------|",
        ]

        for result in sorted(all_results, key=lambda x: x["task_id"]):
            task_id = result["task_id"]
            text_eval = result.get("text_evaluation", {})
            img_eval = result.get("image_evaluation", {})
            summary = result.get("summary", {})

            overall_score = text_eval.get("overall_score", 0)
            images_present = img_eval.get("all_images_present", False)
            passed = summary.get("overall_passed", False)
            feedback = text_eval.get("llm_feedback", "")[:50]  # ì²˜ìŒ 50ìë§Œ

            status_icon = "âœ…" if passed else "âŒ"
            image_icon = "âœ…" if images_present else "âš ï¸"

            report_lines.append(
                f"| {task_id} | {overall_score:.1f} | {overall_score:.1f} | {image_icon} | {status_icon} | {feedback}... |"
            )

        report_lines.extend(
            [
                "",
                "---",
                "",
                "## ì„¸ë¶€ í†µê³„",
                "",
                "### ì ìˆ˜ ë¶„í¬",
                "",
            ]
        )

        # ì ìˆ˜ ë¶„í¬ ê³„ì‚°
        score_ranges = {"90-100": 0, "80-89": 0, "70-79": 0, "60-69": 0, "0-59": 0}
        for result in all_results:
            score = result.get("text_evaluation", {}).get("overall_score", 0)
            if score >= 90:
                score_ranges["90-100"] += 1
            elif score >= 80:
                score_ranges["80-89"] += 1
            elif score >= 70:
                score_ranges["70-79"] += 1
            elif score >= 60:
                score_ranges["60-69"] += 1
            else:
                score_ranges["0-59"] += 1

        for range_name, count in score_ranges.items():
            report_lines.append(f"- **{range_name}ì **: {count}ê°œ íƒœìŠ¤í¬")

        report_lines.extend(
            [
                "",
                "### ì´ë¯¸ì§€ í‰ê°€",
                "",
            ]
        )

        # ì´ë¯¸ì§€ í†µê³„
        tasks_with_images = sum(
            1 for r in all_results if len(r.get("image_evaluation", {}).get("expected_images", [])) > 0
        )
        tasks_all_images_present = sum(
            1 for r in all_results if r.get("image_evaluation", {}).get("all_images_present", False)
        )

        report_lines.append(f"- **ì´ë¯¸ì§€ í¬í•¨ íƒœìŠ¤í¬**: {tasks_with_images}ê°œ")
        report_lines.append(
            f"- **ëª¨ë“  ì´ë¯¸ì§€ ìƒì„± ì„±ê³µ**: {tasks_all_images_present}/{tasks_with_images}ê°œ"
        )

        # SSIM í‰ê· 
        ssim_scores = [
            r.get("image_evaluation", {}).get("average_similarity")
            for r in all_results
            if r.get("image_evaluation", {}).get("average_similarity") is not None
        ]
        if ssim_scores:
            avg_ssim = sum(ssim_scores) / len(ssim_scores)
            report_lines.append(f"- **í‰ê·  ì´ë¯¸ì§€ ìœ ì‚¬ë„ (SSIM)**: {avg_ssim:.3f}")

        report_lines.extend(["", "---", "", f"*ë¦¬í¬íŠ¸ ìƒì„±: HITS AI Agent QA System*", ""])

        # íŒŒì¼ë¡œ ì €ì¥
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))

        print(f"Summary report saved to: {output_path}")

    def load_task_report(self, report_path: Path) -> Dict[str, Any]:
        """
        íƒœìŠ¤í¬ ë¦¬í¬íŠ¸ ë¡œë“œ

        Args:
            report_path: ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ

        Returns:
            ë¦¬í¬íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        with open(report_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def print_task_summary(self, report: Dict[str, Any]) -> None:
        """
        íƒœìŠ¤í¬ ë¦¬í¬íŠ¸ ìš”ì•½ ì¶œë ¥

        Args:
            report: ë¦¬í¬íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*60}")
        print(f"Task: {report['task_id']}")
        print(f"{'='*60}")

        text_eval = report.get("text_evaluation", {})
        print(f"\nğŸ“Š Text Evaluation:")
        print(f"  - Overall Score: {text_eval.get('overall_score', 0):.1f}/100")
        print(f"  - Content Accuracy: {text_eval.get('scores', {}).get('content_accuracy', 0):.1f}")
        print(f"  - Completeness: {text_eval.get('scores', {}).get('completeness', 0):.1f}")
        print(f"  - Format Compliance: {text_eval.get('scores', {}).get('format_compliance', 0):.1f}")
        print(f"  - Passed: {'âœ… Yes' if text_eval.get('passed', False) else 'âŒ No'}")

        img_eval = report.get("image_evaluation", {})
        print(f"\nğŸ–¼ï¸  Image Evaluation:")
        print(f"  - Expected Images: {len(img_eval.get('expected_images', []))}")
        print(f"  - Found Images: {len(img_eval.get('found_images', []))}")
        print(f"  - Missing Images: {img_eval.get('missing_images', [])}")
        print(
            f"  - All Images Present: {'âœ… Yes' if img_eval.get('all_images_present', False) else 'âŒ No'}"
        )
        if img_eval.get("average_similarity") is not None:
            print(f"  - Average SSIM: {img_eval.get('average_similarity', 0):.3f}")

        print(f"\nğŸ’¬ LLM Feedback:")
        print(f"  {text_eval.get('llm_feedback', 'No feedback')}")

        summary = report.get("summary", {})
        print(f"\n{'='*60}")
        print(
            f"Overall Result: {'âœ… PASSED' if summary.get('overall_passed', False) else 'âŒ FAILED'}"
        )
        print(f"{'='*60}\n")

