#!/usr/bin/env python3
"""
HITS AI Agent QA Single Task Runner
ë‹¨ì¼ task/attemptë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import json
import os
import re
import shutil
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

# Biomni HITS ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from biomni.config import default_config
from biomni.agent import A1_HITS
from biomni.llm import get_llm

# default_config ì„¤ì •
default_config.llm = "gemini-3-pro-preview"
default_config.commercial_mode = True
default_config.use_tool_retriever = True
default_config.path = "/workdir_efs/jaechang/work2/biomni_hits_test/biomni_data"
default_config.timeout_seconds = 3600

from qa_core import QAManager, Evaluator, ImageComparator


def extract_solution_from_response(response: str) -> str:
    """
    AI agent ì‘ë‹µì—ì„œ <solution>...</solution> íƒœê·¸ ë‚´ìš© ì¶”ì¶œ

    Args:
        response: AI agentì˜ ì „ì²´ ì‘ë‹µ

    Returns:
        solution íƒœê·¸ ë‚´ìš© (íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µ ë°˜í™˜)
    """
    # <solution>...</solution> íŒ¨í„´ ì¶”ì¶œ
    pattern = r"<solution>(.*?)</solution>"
    match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)

    if match:
        solution_content = match.group(1).strip()
        print(f"âœ… Extracted solution content ({len(solution_content)} chars)")
        return solution_content
    else:
        # solution íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µ ë°˜í™˜
        print("âš ï¸  <solution> tag not found. Using full response.")
        return response.strip()


def run_single_task(
    task_id: str,
    attempt_num: int,
    total_attempts: int,
    qa_datasets_dir: Path,
    output_dir: Path,
    pass_threshold: float,
    ssim_threshold: float,
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ íƒœìŠ¤í¬ ì‹¤í–‰ ë° í‰ê°€

    Args:
        task_id: íƒœìŠ¤í¬ ID
        attempt_num: í˜„ì¬ ì‹œë„ ë²ˆí˜¸
        total_attempts: ì´ ì‹œë„ íšŸìˆ˜
        qa_datasets_dir: QA ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        pass_threshold: í†µê³¼ ê¸°ì¤€ ì ìˆ˜
        ssim_threshold: SSIM ì„ê³„ê°’

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\n{'='*80}")
    print(f"ğŸš€ Starting Task: {task_id} (Attempt {attempt_num}/{total_attempts})")
    print(f"{'='*80}\n")

    start_time = time.time()

    # 1. QA Managerë¡œ íƒœìŠ¤í¬ ë¡œë“œ
    print("ğŸ“¦ Loading task...")
    qa_manager = QAManager(qa_datasets_dir)
    task = qa_manager.get_task(task_id)

    if task is None:
        error_msg = f"Task {task_id} not found!"
        print(f"âŒ {error_msg}")
        return {
            "task_id": task_id,
            "attempt_num": attempt_num,
            "summary": {
                "overall_passed": False,
                "error": error_msg,
            },
            "execution_time": 0,
        }

    print(f"âœ… Task loaded: {task.task_id}")
    print(f"   Category: {task.category}")
    print(f"   Difficulty: {task.difficulty}")
    print(f"   Images: {len(task.images)}")
    print(f"   Input Data: {len(task.input_data)}")

    # 2. ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    task_output_dir = output_dir / task_id / f"attempt_{attempt_num}"
    task_output_dir.mkdir(parents=True, exist_ok=True)

    # 2.1. Input data íŒŒì¼ë“¤ì„ ì‘ì—… ë””ë ‰í† ë¦¬ì— ë³µì‚¬
    if task.input_data:
        print(f"\nğŸ“¥ Copying input data files...")
        for data_file in task.input_data:
            src_path = task.task_path / data_file
            dst_path = task_output_dir / data_file
            if src_path.exists():
                dst_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(src_path, dst_path)
                print(f"   âœ“ Copied: {data_file}")
            else:
                print(f"   âš ï¸  Not found: {data_file}")

    # 3. HITS ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    print("\nğŸ¤– Initializing HITS Agent...")
    try:
        agent = A1_HITS()
        print("âœ… Agent initialized")
    except Exception as e:
        error_msg = f"Failed to initialize agent: {str(e)}"
        print(f"âŒ {error_msg}")
        import traceback

        traceback.print_exc()
        return {
            "task_id": task_id,
            "attempt_num": attempt_num,
            "summary": {
                "overall_passed": False,
                "error": error_msg,
            },
            "execution_time": time.time() - start_time,
        }

    # 4. ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ task_output_dirë¡œ ë³€ê²½)
    print(f"\nğŸ”„ Running agent with question...")
    print(f"Question preview: {task.question[:200]}...")

    # í˜„ì¬ ë””ë ‰í† ë¦¬ ì €ì¥ ë° ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½
    original_dir = os.getcwd()
    os.chdir(task_output_dir)
    print(f"ğŸ“‚ Changed working directory to: {task_output_dir}")

    try:
        # ì—ì´ì „íŠ¸ì—ê²Œ ì§ˆë¬¸ ì „ë‹¬ (generator ë°˜í™˜)
        agent_start_time = time.time()

        # agent.go()ëŠ” generatorë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ iterateí•´ì•¼ í•¨
        full_response_parts = []
        system_prompt = None
        logs_content = []  # Stepë³„ ë¡œê·¸ ì €ì¥

        print("ğŸ¤– Agent is thinking...\n")

        for idx, output in enumerate(agent.go(task.question)):
            print(f"==================== Step {idx} ====================")

            if idx == 0:
                # ì²« ë²ˆì§¸ ì¶œë ¥ì€ system prompt
                system_prompt = output
                print("System prompt loaded")
                continue

            # Handle structured content (list with images) - extract text only
            if isinstance(output, list):
                # Extract text parts from structured content
                text_parts = [
                    item["text"]
                    for item in output
                    if isinstance(item, dict) and item.get("type") == "text"
                ]
                if text_parts:
                    text_content = "\n".join(text_parts)
                    full_response_parts.extend(text_parts)
                    logs_content.append(text_content)
                    logs_content.append(
                        f"===================={idx}===================="
                    )
            elif isinstance(output, str):
                full_response_parts.append(output)
                logs_content.append(output)
                logs_content.append(f"===================={idx}====================")

        # ì „ì²´ ì‘ë‹µ ì¡°í•©
        full_response = "\n".join(full_response_parts)
        agent_execution_time = time.time() - agent_start_time

        print(f"\nâœ… Agent completed in {agent_execution_time:.1f}s")
        print(f"   Total steps: {idx}")

        # Agent timer ì •ë³´ ì¶œë ¥
        if hasattr(agent, "timer"):
            print(f"   Agent timer: {agent.timer}")

        # <solution> íƒœê·¸ì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ
        final_answer = extract_solution_from_response(full_response)

        print(f"   Full response length: {len(full_response)} characters")
        print(f"   Final answer length: {len(final_answer)} characters")
        print(f"\nFinal answer preview: {final_answer[:200]}...")

        # ì „ì²´ ì‘ë‹µ ì €ì¥ (full_response.md)
        full_response_file = task_output_dir / "full_response.md"
        with open(full_response_file, "w", encoding="utf-8") as f:
            f.write(full_response)
        print(f"ğŸ“„ Full response saved to: {full_response_file}")

        # ìµœì¢… ë‹µë³€ ì €ì¥ (agent_answer.md) - solution íƒœê·¸ ì¶”ì¶œëœ ë‚´ìš©
        answer_file = task_output_dir / "agent_answer.md"
        with open(answer_file, "w", encoding="utf-8") as f:
            f.write(final_answer)
        print(f"ğŸ“„ Final answer saved to: {answer_file}")

        # System promptë„ ì €ì¥ (ë””ë²„ê¹…ìš©)
        if system_prompt:
            system_prompt_file = task_output_dir / "system_prompt.txt"
            with open(system_prompt_file, "w", encoding="utf-8") as f:
                if isinstance(system_prompt, str):
                    f.write(system_prompt)
                else:
                    f.write(str(system_prompt))
            print(f"ğŸ“„ System prompt saved to: {system_prompt_file}")

        # Stepë³„ ë¡œê·¸ ì €ì¥ (ë””ë²„ê¹…ìš©)
        if logs_content:
            logs_file = task_output_dir / "logs.txt"
            with open(logs_file, "w", encoding="utf-8") as f:
                f.write("\n".join(logs_content))
            print(f"ğŸ“„ Step logs saved to: {logs_file}")

        # í‰ê°€ì—ëŠ” ìµœì¢… ë‹µë³€ ì‚¬ìš©
        answer = final_answer

    except Exception as e:
        error_msg = f"Agent execution failed: {str(e)}"
        print(f"âŒ {error_msg}")
        import traceback

        traceback.print_exc()
        return {
            "task_id": task_id,
            "attempt_num": attempt_num,
            "summary": {
                "overall_passed": False,
                "error": error_msg,
            },
            "execution_time": time.time() - start_time,
        }
    finally:
        # ì›ë˜ ë””ë ‰í† ë¦¬ë¡œ ë³µê·€
        os.chdir(original_dir)
        print(f"ğŸ“‚ Restored working directory to: {original_dir}")

    # 5. LLM ê¸°ë°˜ ë‹µë³€ í‰ê°€
    print(f"\nğŸ“Š Evaluating answer with LLM...")
    try:
        llm_client = get_llm(model="gemini-3-pro-preview")
        evaluation_prompt_path = (
            Path(__file__).parent / "qa_config" / "evaluation_prompt.txt"
        )
        evaluator = Evaluator(
            llm_client=llm_client,
            pass_threshold=pass_threshold,
            evaluation_prompt_path=(
                str(evaluation_prompt_path) if evaluation_prompt_path.exists() else None
            ),
        )

        eval_result = evaluator.evaluate_answer(
            task_id=task_id,
            question=task.question,
            ground_truth=task.answer,
            generated_answer=answer,
        )

        print(f"âœ… LLM Evaluation:")
        print(f"   Content Accuracy: {eval_result.scores['content_accuracy']:.1f}")
        print(f"   Completeness: {eval_result.scores['completeness']:.1f}")
        print(f"   Format Compliance: {eval_result.scores['format_compliance']:.1f}")
        print(f"   Overall Score: {eval_result.overall_score:.1f}")
        print(f"   Passed: {'âœ…' if eval_result.passed else 'âŒ'}")
        print(f"   Feedback: {eval_result.llm_feedback}")

    except Exception as e:
        error_msg = f"LLM evaluation failed: {str(e)}"
        print(f"âŒ {error_msg}")
        import traceback

        traceback.print_exc()
        # Continue with dummy evaluation
        eval_result = None

    # 6. ì´ë¯¸ì§€ ë¹„êµ (ìˆëŠ” ê²½ìš°)
    image_eval_result = None
    if task.images:
        print(f"\nğŸ–¼ï¸  Evaluating images...")
        try:
            # Vision-capable LLM ì‚¬ìš©
            vision_llm = get_llm(model="gemini-3-pro-preview")

            image_comparator = ImageComparator(
                ssim_threshold=ssim_threshold,
                llm_client=vision_llm,
                llm_threshold=pass_threshold,
            )

            # ì´ë¯¸ì§€ í‰ê°€ ìˆ˜í–‰
            image_eval_result = image_comparator.evaluate_images(
                ground_truth_markdown=task.answer,
                generated_markdown=answer,
                ground_truth_task_dir=task.task_path,
                generated_task_dir=task_output_dir,
                question=task.question,
                compare_visually=True,
                use_llm_comparison=True,
            )

            print(f"âœ… Image Evaluation:")
            print(f"   Expected: {len(image_eval_result.expected_images)} images")
            print(f"   Found: {len(image_eval_result.found_images)} images")
            print(f"   Missing: {len(image_eval_result.missing_images)} images")
            print(
                f"   All Present: {'âœ…' if image_eval_result.all_images_present else 'âŒ'}"
            )

            if image_eval_result.average_similarity is not None:
                print(f"   Avg SSIM: {image_eval_result.average_similarity:.3f}")

            if image_eval_result.average_llm_score is not None:
                print(f"   LLM Image Score: {image_eval_result.average_llm_score:.1f}")

        except Exception as e:
            error_msg = f"Image evaluation failed: {str(e)}"
            print(f"âš ï¸  {error_msg}")
            import traceback

            traceback.print_exc()

    # 7. ìµœì¢… í‰ê°€ ê²°ê³¼ ìƒì„±
    execution_time = time.time() - start_time

    # í†µê³¼ ì¡°ê±´ íŒë‹¨
    text_passed = eval_result.passed if eval_result else False

    # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ëª¨ë‘ í†µê³¼í•´ì•¼ í•¨
    # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°: í…ìŠ¤íŠ¸ë§Œ í†µê³¼í•˜ë©´ ë¨
    if task.images:
        # ì´ë¯¸ì§€ í‰ê°€ í†µê³¼ ì¡°ê±´
        images_passed = False
        if image_eval_result:
            # 1. ëª¨ë“  ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ê±°ë‚˜
            # 2. LLMì´ ë‚´ìš©ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  íŒë‹¨í•˜ê±°ë‚˜
            # 3. SSIM í‰ê· ì´ ì„ê³„ê°’ ì´ìƒì´ë©´ í†µê³¼
            if image_eval_result.all_images_present:
                images_passed = True
            elif (
                image_eval_result.average_similarity is not None
                and image_eval_result.average_similarity >= ssim_threshold
            ):
                images_passed = True
            elif (
                image_eval_result.average_llm_score is not None
                and image_eval_result.average_llm_score >= pass_threshold
            ):
                images_passed = True

        overall_passed = text_passed and images_passed
    else:
        overall_passed = text_passed

    final_result = {
        "task_id": task_id,
        "attempt_num": attempt_num,
        "timestamp": datetime.now().isoformat(),
        "execution_time": execution_time,
        "agent_execution_time": (
            agent_execution_time if "agent_execution_time" in locals() else 0
        ),
        "summary": {
            "overall_passed": overall_passed,
            "text_evaluation_passed": text_passed,
            "image_evaluation_passed": images_passed if task.images else None,
        },
        "text_evaluation": eval_result.to_dict() if eval_result else None,
        "image_evaluation": image_eval_result.to_dict() if image_eval_result else None,
        "metadata": {
            "category": task.category,
            "difficulty": task.difficulty,
            "num_expected_images": len(task.images),
            "num_input_data": len(task.input_data),
        },
    }

    # 8. ê²°ê³¼ ì €ì¥
    evaluation_file = task_output_dir / "evaluation.json"
    with open(evaluation_file, "w", encoding="utf-8") as f:
        json.dump(final_result, f, indent=2, ensure_ascii=False)

    print(f"\n{'='*80}")
    print(
        f"{'âœ… PASSED' if overall_passed else 'âŒ FAILED'}: {task_id} (Attempt {attempt_num}/{total_attempts})"
    )
    print(f"{'='*80}")
    print(f"â±ï¸  Execution Time: {execution_time:.1f}s")
    print(f"ğŸ“„ Evaluation saved to: {evaluation_file}")
    print()

    return final_result


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="HITS AI Agent QA Single Task Runner")

    parser.add_argument(
        "--task-id",
        type=str,
        required=True,
        help="Task ID to run",
    )

    parser.add_argument(
        "--attempt",
        type=int,
        required=True,
        help="Attempt number",
    )

    parser.add_argument(
        "--total-attempts",
        type=int,
        required=True,
        help="Total number of attempts",
    )

    parser.add_argument(
        "--qa-datasets-dir",
        type=str,
        required=True,
        help="QA datasets directory",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        required=True,
        help="Output directory",
    )

    parser.add_argument(
        "--pass-threshold",
        type=float,
        default=70.0,
        help="Pass threshold score (0-100, default: 70)",
    )

    parser.add_argument(
        "--ssim-threshold",
        type=float,
        default=0.8,
        help="SSIM threshold for image comparison (0-1, default: 0.8)",
    )

    args = parser.parse_args()

    # ê²½ë¡œ ë³€í™˜
    qa_datasets_dir = Path(args.qa_datasets_dir)
    output_dir = Path(args.output_dir)

    # ì‹¤í–‰
    result = run_single_task(
        task_id=args.task_id,
        attempt_num=args.attempt,
        total_attempts=args.total_attempts,
        qa_datasets_dir=qa_datasets_dir,
        output_dir=output_dir,
        pass_threshold=args.pass_threshold,
        ssim_threshold=args.ssim_threshold,
    )

    # ê²°ê³¼ì— ë”°ë¼ exit code ì„¤ì •
    exit_code = 0 if result.get("summary", {}).get("overall_passed", False) else 1
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
